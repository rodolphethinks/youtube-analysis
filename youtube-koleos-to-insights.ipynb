{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get 100 Relevant YouTube URLs\n",
    "About 30s for 250 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:54:37.954410Z",
     "iopub.status.busy": "2025-07-08T05:54:37.954202Z",
     "iopub.status.idle": "2025-07-08T05:54:54.494179Z",
     "shell.execute_reply": "2025-07-08T05:54:54.493412Z",
     "shell.execute_reply.started": "2025-07-08T05:54:37.954391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique videos found: 107\n",
      "CPU times: total: 22.5 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import re\n",
    "\n",
    "# Function to fetch YouTube video URLs based on search query\n",
    "def get_youtube_videos(query, max_results=1000):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=max_results,\n",
    "        q=query,\n",
    "        type=\"video\",\n",
    "        publishedAfter=\"2024-04-01T00:00:00Z\",\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return {f\"https://www.youtube.com/watch?v={item['id']['videoId']}\": item['id']['videoId'] for item in response['items']}  # Return dict with URLs as keys and IDs as values\n",
    "\n",
    "# Function to fetch detailed information for a video using video ID\n",
    "def get_video_details(video_id, video_url):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if not response['items']:\n",
    "        return None\n",
    "    \n",
    "    video_info = response['items'][0]\n",
    "    \n",
    "    # Extracting data from the response\n",
    "    title = video_info['snippet']['title']\n",
    "    release_date = video_info['snippet']['publishedAt']\n",
    "    channel_id = video_info['snippet'].get('channelId', 'N/A')\n",
    "    channel_title = video_info['snippet'].get('channelTitle', 'N/A')\n",
    "    views = int(video_info['statistics'].get('viewCount', 0))\n",
    "    likes = int(video_info['statistics'].get('likeCount', 0))\n",
    "    comments = int(video_info['statistics'].get('commentCount', 0))\n",
    "    duration = video_info['contentDetails'].get('duration', 'N/A')\n",
    "    \n",
    "    return {\n",
    "        'Video URL': video_url,\n",
    "        'Title': title,\n",
    "        'Release Date': pd.to_datetime(release_date).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Channel ID': channel_id,\n",
    "        'Channel Title': channel_title,\n",
    "        'Views': views,\n",
    "        'Likes': likes,\n",
    "        'Comments': comments,\n",
    "        'Duration': duration\n",
    "    }\n",
    "\n",
    "def convert_duration(duration):\n",
    "    match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', duration)\n",
    "    hours = int(match.group(1)) if match.group(1) else 0\n",
    "    minutes = int(match.group(2)) if match.group(2) else 0\n",
    "    seconds = int(match.group(3)) if match.group(3) else 0\n",
    "    return f'{hours:02}:{minutes:02}:{seconds:02}'\n",
    "\n",
    "def car_videos(car_company, car_model):\n",
    "    # Define different search queries\n",
    "    # search_queries = [\n",
    "    #     f\"{car_model} {car_company}\",\n",
    "    #     f\"{car_model} 시승기\",\n",
    "    #     f\"{car_company} {car_model} 신차 리뷰\",\n",
    "    #     f\"{car_model} 하이브리드 장단점\",\n",
    "    #     f\"{car_model} 가격 옵션\",\n",
    "    # ]\n",
    "\n",
    "    search_queries = [\n",
    "    f\"{car_model} {car_company}\",\n",
    "    f\"{car_model} 전기차 시승기\",\n",
    "    f\"{car_company} {car_model} 전기차 리뷰\",\n",
    "    f\"{car_model} 충전 속도 장단점\",\n",
    "    f\"{car_model} 가격 옵션 사양\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Get unique video URLs and IDs across all searches\n",
    "    video_dict = {}\n",
    "    for query in search_queries:\n",
    "        video_dict.update(get_youtube_videos(query))  # Add results while avoiding duplicates\n",
    "    \n",
    "    # Get detailed information about each video\n",
    "    video_details = []\n",
    "    for video_url, video_id in video_dict.items():\n",
    "        details = get_video_details(video_id, video_url)\n",
    "        if details:\n",
    "            video_details.append(details)\n",
    "    \n",
    "    # Convert to a pandas DataFrame for easier handling\n",
    "    df = pd.DataFrame(video_details)\n",
    "    df['Release Date'] = pd.to_datetime(df['Release Date']).dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df = df[df['Title'].str.contains('|'.join(car_model.split(\" \") + car_company.split(\" \")), case=False, na=False)]\n",
    "    df = df.sort_values(by='Views', ascending=False)\n",
    "    df['Duration'] = df['Duration'].apply(convert_duration)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(f\"Total unique videos found: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# car_company_koleos = \"르노 코리아\"\n",
    "# car_model_koleos = \"그랑 콜레오스\"\n",
    "# koleos_df = car_videos(car_company_koleos, car_model_koleos)\n",
    "\n",
    "# car_company_sorento = \"기아\"\n",
    "# car_model_sorento = \"쏘렌토\"\n",
    "# sorento_df = car_videos(car_company_sorento, car_model_sorento)\n",
    "\n",
    "# car_company_santafe = \"현대\"\n",
    "# car_model_santafe = \"싼타페\"\n",
    "# santafe_df = car_videos(car_company_santafe, car_model_santafe)\n",
    "\n",
    "# car_company_torres = \"토레스 하이브리드\"\n",
    "# car_model_torres = \"KGM\"\n",
    "# torres_df = car_videos(car_company_torres, car_model_torres)\n",
    "\n",
    "car_company_scenic = \"르노\"\n",
    "car_model_scenic = \"Scenic E-Tech\"\n",
    "scenic_df = car_videos(car_company_scenic, car_model_scenic)\n",
    "# scenic_df = scenic_df[scenic_df['Title'].str.contains(r\"(?=.*르노)(?=.*세닉)\", na=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:54:54.495766Z",
     "iopub.status.busy": "2025-07-08T05:54:54.495357Z",
     "iopub.status.idle": "2025-07-08T05:54:54.500448Z",
     "shell.execute_reply": "2025-07-08T05:54:54.499768Z",
     "shell.execute_reply.started": "2025-07-08T05:54:54.495733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scenic_df_filtered = scenic_df[scenic_df['Title'].str.contains(r\"(?=.*르노)(?=.*세닉)\", na=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:54:54.502114Z",
     "iopub.status.busy": "2025-07-08T05:54:54.501796Z",
     "iopub.status.idle": "2025-07-08T05:54:54.521332Z",
     "shell.execute_reply": "2025-07-08T05:54:54.520566Z",
     "shell.execute_reply.started": "2025-07-08T05:54:54.502075Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scenic_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:54:54.522606Z",
     "iopub.status.busy": "2025-07-08T05:54:54.522309Z",
     "iopub.status.idle": "2025-07-08T05:55:05.334434Z",
     "shell.execute_reply": "2025-07-08T05:55:05.333512Z",
     "shell.execute_reply.started": "2025-07-08T05:54:54.522585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments for video ZvFEIk-HUGE: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=ZvFEIk-HUGE&maxResults=100&order=relevance&textFormat=plainText&key=AIzaSyApPx4FGEl65-tsH-sIGtcELBBdDxONOiU&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Total comments collected: 1613\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Published At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HPDI11MoU1Y</td>\n",
       "      <td>@ncnccc</td>\n",
       "      <td>진짜 잘 나온 거 같다</td>\n",
       "      <td>29</td>\n",
       "      <td>2024-04-04 03:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HPDI11MoU1Y</td>\n",
       "      <td>@tekjin8058</td>\n",
       "      <td>디자인 아주 멋집니다.</td>\n",
       "      <td>76</td>\n",
       "      <td>2024-04-03 23:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HPDI11MoU1Y</td>\n",
       "      <td>@keemzuno</td>\n",
       "      <td>휠 겁내 이쁘네..</td>\n",
       "      <td>45</td>\n",
       "      <td>2024-04-03 23:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HPDI11MoU1Y</td>\n",
       "      <td>@제루-v1v</td>\n",
       "      <td>1:11  주상절리인줄</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-04-03 23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HPDI11MoU1Y</td>\n",
       "      <td>@ZshCenturion</td>\n",
       "      <td>문제는 가격이겠네요....\\n보조금 맥스 가격선으로 책정하게 되면 다른 업체에 밀릴...</td>\n",
       "      <td>26</td>\n",
       "      <td>2024-04-04 00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video ID         Author  \\\n",
       "0  HPDI11MoU1Y        @ncnccc   \n",
       "1  HPDI11MoU1Y    @tekjin8058   \n",
       "2  HPDI11MoU1Y      @keemzuno   \n",
       "3  HPDI11MoU1Y        @제루-v1v   \n",
       "4  HPDI11MoU1Y  @ZshCenturion   \n",
       "\n",
       "                                             Comment  Likes      Published At  \n",
       "0                                       진짜 잘 나온 거 같다     29  2024-04-04 03:29  \n",
       "1                                       디자인 아주 멋집니다.     76  2024-04-03 23:25  \n",
       "2                                         휠 겁내 이쁘네..     45  2024-04-03 23:12  \n",
       "3                                       1:11  주상절리인줄     10  2024-04-03 23:59  \n",
       "4  문제는 가격이겠네요....\\n보조금 맥스 가격선으로 책정하게 되면 다른 업체에 밀릴...     26  2024-04-04 00:17  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_video_comments(video_id, max_comments=100):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Max per request\n",
    "            order=\"relevance\",  # Get top comments\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            author = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "            likes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "            published_at = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "\n",
    "            comments.append({\n",
    "                \"Video ID\": video_id,\n",
    "                \"Author\": author,\n",
    "                \"Comment\": comment,\n",
    "                \"Likes\": likes,\n",
    "                \"Published At\": pd.to_datetime(published_at).strftime('%Y-%m-%d %H:%M')\n",
    "            })\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Fetch comments for all videos in scenic_df_filtered\n",
    "all_comments = []\n",
    "for video_id in scenic_df_filtered[\"Video URL\"].str.split(\"v=\").str[-1]:  # Extract video ID from URL\n",
    "    comments = get_video_comments(video_id)\n",
    "    all_comments.extend(comments)\n",
    "\n",
    "# Convert comments to DataFrame\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "print(f\"Total comments collected: {len(comments_df)}\")\n",
    "\n",
    "# Save to CSV if needed\n",
    "comments_df.to_csv(\"scenic_comments.csv\", index=False)\n",
    "\n",
    "# Display sample\n",
    "comments_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:55:05.335697Z",
     "iopub.status.busy": "2025-07-08T05:55:05.335389Z",
     "iopub.status.idle": "2025-07-08T05:55:05.340614Z",
     "shell.execute_reply": "2025-07-08T05:55:05.339838Z",
     "shell.execute_reply.started": "2025-07-08T05:55:05.335665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1613"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transcribe YouTube Videos (10 mins for 15 videos)\n",
    "### 165 videos: 1H10\n",
    "**To-do:** Delete .wav file once transcripted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:55:05.341806Z",
     "iopub.status.busy": "2025-07-08T05:55:05.341500Z",
     "iopub.status.idle": "2025-07-08T05:55:05.358487Z",
     "shell.execute_reply": "2025-07-08T05:55:05.357915Z",
     "shell.execute_reply.started": "2025-07-08T05:55:05.341776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:55:05.359498Z",
     "iopub.status.busy": "2025-07-08T05:55:05.359233Z",
     "iopub.status.idle": "2025-07-08T05:55:42.674048Z",
     "shell.execute_reply": "2025-07-08T05:55:42.673155Z",
     "shell.execute_reply.started": "2025-07-08T05:55:05.359472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=API_KEY)\n",
    "gen_model = genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T05:55:42.676044Z",
     "iopub.status.busy": "2025-07-08T05:55:42.675686Z",
     "iopub.status.idle": "2025-07-08T05:56:39.644497Z",
     "shell.execute_reply": "2025-07-08T05:56:39.643726Z",
     "shell.execute_reply.started": "2025-07-08T05:55:42.676022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and pipeline loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/69 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "c:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import yt_dlp\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to download audio from YouTube using yt-dlp\n",
    "def download_audio_from_youtube(url, output_path='downloads'):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': f'{output_path}/%(id)s.%(ext)s',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "        }],\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "        'cookies-from-browser': 'chrome'\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.cache.remove()\n",
    "        info_dict = ydl.extract_info(url, download=True)\n",
    "        audio_file = f\"{output_path}/{info_dict['id']}.wav\"\n",
    "        return audio_file\n",
    "\n",
    "\n",
    "# Function to transcribe a list of video URLs with a silent loading bar\n",
    "def transcribe_videos(video_urls):\n",
    "    transcriptions = {}\n",
    "    \n",
    "    with tqdm(total=len(video_urls), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                audio_file = download_audio_from_youtube(url)\n",
    "                decision = gen_model.generate_content(f'For the following YouTube audio, you will give a boolean answer whether the transcription is well done, or is likely that an issue occurred. Only answer True if well done and False otherwise: {transcription}').text.strip()\n",
    "                \n",
    "                if decision == 'True':\n",
    "                    transcriptions[url] = transcription\n",
    "                    #os.remove(audio_file)\n",
    "                    break\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    print(f\"Retrying transcription for {url}. Attempt {retries}/3\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                #os.remove(audio_file)\n",
    "                print(f\"Error processing {url}: {str(e)}\")\n",
    "                break\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    return transcriptions\n",
    "\n",
    "# Load the model and pipeline\n",
    "pipeline = load_pipeline()\n",
    "\n",
    "# transcriptions_koleos = transcribe_videos(koleos_df[\"Video URL\"].tolist()[:20])\n",
    "# transcriptions_sorento = transcribe_videos(sorento_df[\"Video URL\"].tolist()[:20])\n",
    "# transcriptions_santafe = transcribe_videos(santafe_df[\"Video URL\"].tolist()[:20])\n",
    "# transcriptions_torres = transcribe_videos(torres_df[\"Video URL\"].tolist())\n",
    "transcriptions_scenic = transcribe_videos(scenic_df_filtered[\"Video URL\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
      "[youtube] dQw4w9WgXcQ: Downloading webpage\n",
      "[youtube] dQw4w9WgXcQ: Downloading tv client config\n",
      "[youtube] dQw4w9WgXcQ: Downloading tv player API JSON\n",
      "[youtube] dQw4w9WgXcQ: Downloading ios player API JSON\n",
      "[youtube] dQw4w9WgXcQ: Downloading m3u8 information\n",
      "[info] dQw4w9WgXcQ: Downloading 1 format(s): 251\n",
      "[download] Destination: audio.wav\n",
      "[download] 100% of    3.27MiB in 00:00:00 at 6.70MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Postprocessing: ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location\n"
     ]
    },
    {
     "ename": "DownloadError",
     "evalue": "ERROR: Postprocessing: ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPostProcessingError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3596\u001b[39m, in \u001b[36mYoutubeDL.process_info\u001b[39m\u001b[34m(self, info_dict)\u001b[39m\n\u001b[32m   3595\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3596\u001b[39m     replace_info_dict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles_to_move\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   3597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PostProcessingError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3780\u001b[39m, in \u001b[36mYoutubeDL.post_process\u001b[39m\u001b[34m(self, filename, info, files_to_move)\u001b[39m\n\u001b[32m   3779\u001b[39m info[\u001b[33m'\u001b[39m\u001b[33m__files_to_move\u001b[39m\u001b[33m'\u001b[39m] = files_to_move \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m3780\u001b[39m info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_all_pps\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost_process\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_pps\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__postprocessors\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3781\u001b[39m info = \u001b[38;5;28mself\u001b[39m.run_pp(MoveFilesAfterDownloadPP(\u001b[38;5;28mself\u001b[39m), info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3762\u001b[39m, in \u001b[36mYoutubeDL.run_all_pps\u001b[39m\u001b[34m(self, key, info, additional_pps)\u001b[39m\n\u001b[32m   3761\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pp \u001b[38;5;129;01min\u001b[39;00m (additional_pps \u001b[38;5;129;01mor\u001b[39;00m []) + \u001b[38;5;28mself\u001b[39m._pps[key]:\n\u001b[32m-> \u001b[39m\u001b[32m3762\u001b[39m     info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_pp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3740\u001b[39m, in \u001b[36mYoutubeDL.run_pp\u001b[39m\u001b[34m(self, pp, infodict)\u001b[39m\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3740\u001b[39m     files_to_delete, infodict = \u001b[43mpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfodict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3741\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PostProcessingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3742\u001b[39m     \u001b[38;5;66;03m# Must be True and not 'only_download'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\postprocessor\\common.py:23\u001b[39m, in \u001b[36mPostProcessorMetaClass.run_wrapper.<locals>.run\u001b[39m\u001b[34m(self, info, *args, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m._hook_progress({\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mstarted\u001b[39m\u001b[33m'\u001b[39m}, info_copy)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\postprocessor\\common.py:128\u001b[39m, in \u001b[36mPostProcessor._restrict_to.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, info)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allowed[format_type]:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\postprocessor\\ffmpeg.py:493\u001b[39m, in \u001b[36mFFmpegExtractAudioPP.run\u001b[39m\u001b[34m(self, information)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], information\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m filecodec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_audio_codec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filecodec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\postprocessor\\ffmpeg.py:241\u001b[39m, in \u001b[36mFFmpegPostProcessor.get_audio_codec\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.probe_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.available:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PostProcessingError(\u001b[33m'\u001b[39m\u001b[33mffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPostProcessingError\u001b[39m: ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDownloadError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      8\u001b[39m ydl_opts = {\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mbestaudio/best\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mouttmpl\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maudio.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     }],\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m yt_dlp.YoutubeDL(ydl_opts) \u001b[38;5;28;01mas\u001b[39;00m ydl:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mydl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43murl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3644\u001b[39m, in \u001b[36mYoutubeDL.download\u001b[39m\u001b[34m(self, url_list)\u001b[39m\n\u001b[32m   3641\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SameFileError(outtmpl)\n\u001b[32m   3643\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[32m-> \u001b[39m\u001b[32m3644\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__download_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3645\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_generic_extractor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforce_generic_extractor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3647\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_retcode\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3617\u001b[39m, in \u001b[36mYoutubeDL.__download_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3614\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   3615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   3616\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3617\u001b[39m         res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3618\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m CookieLoadError:\n\u001b[32m   3619\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1651\u001b[39m, in \u001b[36mYoutubeDL.extract_info\u001b[39m\u001b[34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ExistingVideoReached\n\u001b[32m   1650\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__extract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_info_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     extractors_restricted = \u001b[38;5;28mself\u001b[39m.params.get(\u001b[33m'\u001b[39m\u001b[33mallowed_extractors\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1662\u001b[39m, in \u001b[36mYoutubeDL._handle_extraction_exceptions.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1661\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1662\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1663\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (CookieLoadError, DownloadCancelled, LazyList.IndexError, PagedList.IndexError):\n\u001b[32m   1664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1818\u001b[39m, in \u001b[36mYoutubeDL.__extract_info\u001b[39m\u001b[34m(self, url, ie, download, extra_info, process)\u001b[39m\n\u001b[32m   1816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process:\n\u001b[32m   1817\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_video(ie_result)\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_ie_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mie_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1820\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ie_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1877\u001b[39m, in \u001b[36mYoutubeDL.process_ie_result\u001b[39m\u001b[34m(self, ie_result, download, extra_info)\u001b[39m\n\u001b[32m   1875\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result_type == \u001b[33m'\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_extra_info(ie_result, extra_info)\n\u001b[32m-> \u001b[39m\u001b[32m1877\u001b[39m     ie_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_video_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mie_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_pending_errors(ie_result)\n\u001b[32m   1879\u001b[39m     additional_urls = (ie_result \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m'\u001b[39m\u001b[33madditional_urls\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3050\u001b[39m, in \u001b[36mYoutubeDL.process_video_result\u001b[39m\u001b[34m(self, info_dict, download)\u001b[39m\n\u001b[32m   3048\u001b[39m downloaded_formats.append(new_info)\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxDownloadsReached:\n\u001b[32m   3052\u001b[39m     max_downloads_reached = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:185\u001b[39m, in \u001b[36m_catch_unsafe_extension_error.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _UnsafeExtensionError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    187\u001b[39m         \u001b[38;5;28mself\u001b[39m.report_error(\n\u001b[32m    188\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe extracted extension (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror.extension\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m) is unusual \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    189\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mand will be skipped for safety reasons. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you believe this is an error\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbug_reports_message(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:3598\u001b[39m, in \u001b[36mYoutubeDL.process_info\u001b[39m\u001b[34m(self, info_dict)\u001b[39m\n\u001b[32m   3596\u001b[39m     replace_info_dict(\u001b[38;5;28mself\u001b[39m.post_process(dl_filename, info_dict, files_to_move))\n\u001b[32m   3597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PostProcessingError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m-> \u001b[39m\u001b[32m3598\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreport_error\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPostprocessing: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43merr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3599\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   3600\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1120\u001b[39m, in \u001b[36mYoutubeDL.report_error\u001b[39m\u001b[34m(self, message, *args, **kwargs)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreport_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, *args, **kwargs):\n\u001b[32m   1116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[33;03m    Do the same as trouble, but prefixes the message with 'ERROR:', colored\u001b[39;00m\n\u001b[32m   1118\u001b[39m \u001b[33;03m    in red if stderr is a tty file.\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrouble\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_err\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mERROR:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mStyles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mERROR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmessage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodol\\OneDrive\\Documents\\Omni\\Youtube\\venv\\Lib\\site-packages\\yt_dlp\\YoutubeDL.py:1059\u001b[39m, in \u001b[36mYoutubeDL.trouble\u001b[39m\u001b[34m(self, message, tb, is_error)\u001b[39m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1058\u001b[39m         exc_info = sys.exc_info()\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DownloadError(message, exc_info)\n\u001b[32m   1060\u001b[39m \u001b[38;5;28mself\u001b[39m._download_retcode = \u001b[32m1\u001b[39m\n",
      "\u001b[31mDownloadError\u001b[39m: ERROR: Postprocessing: ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "\n",
    "# Change this to your desired format\n",
    "target_format = 'wav'  # wav, mp3, aiff, aac, ogg, flac\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': f'audio.{target_format}',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': target_format,\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T00:08:20.323130Z",
     "iopub.status.busy": "2025-03-24T00:08:20.322759Z",
     "iopub.status.idle": "2025-03-24T00:08:58.539003Z",
     "shell.execute_reply": "2025-03-24T00:08:58.538184Z",
     "shell.execute_reply.started": "2025-03-24T00:08:20.323102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transcriptions_torres2 = transcriptions_torres.copy()\n",
    "first_vid = transcribe_videos(['https://www.youtube.com/watch?v=CcCQZqK5cBY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T07:51:38.693679Z",
     "iopub.status.busy": "2025-07-08T07:51:38.693345Z",
     "iopub.status.idle": "2025-07-08T07:51:39.406665Z",
     "shell.execute_reply": "2025-07-08T07:51:39.405969Z",
     "shell.execute_reply.started": "2025-07-08T07:51:38.693652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HTTP Error 400: Bad Request\n"
     ]
    }
   ],
   "source": [
    "youtube_link = \"https://www.youtube.com/watch?v=CcCQZqK5cBY\"\n",
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "def download_audio_pytube(url, output_path=\"./\"):\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "        \n",
    "        # Download the audio\n",
    "        out_file = audio_stream.download(output_path=output_path)\n",
    "        \n",
    "        # Convert to mp3\n",
    "        base, ext = os.path.splitext(out_file)\n",
    "        new_file = base + '.mp3'\n",
    "        os.rename(out_file, new_file)\n",
    "        \n",
    "        print(f\"Downloaded: {yt.title}\")\n",
    "        return new_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "download_audio_pytube(youtube_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T00:10:06.631464Z",
     "iopub.status.busy": "2025-03-24T00:10:06.631156Z",
     "iopub.status.idle": "2025-03-24T00:10:06.635179Z",
     "shell.execute_reply": "2025-03-24T00:10:06.634443Z",
     "shell.execute_reply.started": "2025-03-24T00:10:06.631432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transcriptions_torres.update(first_vid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Analysis with Gemini 2.0 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T03:08:07.773474Z",
     "iopub.status.busy": "2025-03-20T03:08:07.773092Z",
     "iopub.status.idle": "2025-03-20T03:08:07.778083Z",
     "shell.execute_reply": "2025-03-20T03:08:07.777288Z",
     "shell.execute_reply.started": "2025-03-20T03:08:07.773444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def instruct(company, model):   \n",
    "    instruction = (\n",
    "        f\"Analyze the following transcript of a YouTube video discussing the {company} {model}. Write everything in English. \\n\"\n",
    "        \"Do not make any comments that are unrelated to the video to introduce the task. \\n\"\n",
    "        \"Your goal is only to extract key insights and classify the content based on the following criteria:\\n\\n\"\n",
    "        \n",
    "        \"Overall Sentiment: Determine if the video is positive, neutral, or negative toward the {model}. \"\n",
    "        \"Provide a percentage score for your classification, 0 being extremely negative, 50 being neutral, 100 being extremely positive.\\n\"\n",
    "        \n",
    "        \"Key Strengths Highlighted: Identify the main positive aspects the influencer mentions (e.g., design, performance, technology, comfort, pricing).\\n\"\n",
    "        \n",
    "        \"Key Weaknesses Highlighted: Identify the main negative aspects the influencer mentions (e.g., high price, poor fuel economy, lack of features). \"\n",
    "        \"Do not hesitate to point them out.\\n\"\n",
    "        \n",
    "        \"Comments on Renault Brand: Does the influencer mention Renault as a brand? If so, is the sentiment positive, neutral, or negative?\\n\"\n",
    "        \n",
    "        \"Comparison to Competitors: If the video mentions other car brands/models, list them and summarize the comparisons.\\n\"\n",
    "        \n",
    "        \"Trends & Topics (if available, otherwise return only '/'): Identify any recurring themes (e.g., luxury appeal, fuel efficiency, safety features).\\n\"\n",
    "        \n",
    "        \"Influencer’s Overall Verdict: Summarize the influencer’s final thoughts on the {model} in one or two sentences, and Renault Korea's Grand Koleos when applicable.\\n\\n\"\n",
    "        \n",
    "        \"Here is an example of how the output should be formatted where applicable:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"sentiment_analysis\": {\\n'\n",
    "        '    \"overall_sentiment\": \"Positive\",\\n'\n",
    "        '    \"score\": 85\\n'\n",
    "        '  },\\n'\n",
    "        '  \"key_strengths\": [\"Spacious interior\", \"Fuel efficiency\", \"Modern technology\"],\\n'\n",
    "        '  \"key_weaknesses\": [\"Expensive\", \"Limited color options\"],\\n'\n",
    "        '  \"renault_brand_sentiment\": \"Neutral\",\\n'\n",
    "        '  \"competitor_mentions\": [\\n'\n",
    "        '    {\\n'\n",
    "        '      \"competitor\": \"Toyota RAV4\",\\n'\n",
    "        '      \"comparison_summary\": \"Influencer states that Grand Koleos has a better interior but higher price.\"\\n'\n",
    "        '    }\\n'\n",
    "        '  ],\\n'\n",
    "        '  \"trends\": [\"Luxury appeal\", \"Fuel efficiency\"],\\n'\n",
    "        '  \"final_verdict\": \"The influencer believes that the Grand Koleos is a premium SUV with great features but might be slightly overpriced.\"\\n'\n",
    "        '}\\n'\n",
    "    )\n",
    "    \n",
    "    return instruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T07:42:21.793145Z",
     "iopub.status.busy": "2025-03-24T07:42:21.792791Z",
     "iopub.status.idle": "2025-03-24T07:42:21.798298Z",
     "shell.execute_reply": "2025-03-24T07:42:21.797427Z",
     "shell.execute_reply.started": "2025-03-24T07:42:21.793117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def instruct(company, model):   \n",
    "    instruction = (\n",
    "        f'Analyze the following transcript of a YouTube video discussing the {company} {model}. Write everything in English. \\n\"'\n",
    "        \"Do not make any comments that are unrelated to the video to introduce the task. \\n\"\n",
    "        \"Your goal is only to extract key insights and classify the content based on the following criteria:\\n\\n\"\n",
    "        \n",
    "        f\"Overall Sentiment: Determine if the video is positive, neutral, or negative toward the {model}. \"\n",
    "        \"Provide a percentage score for your classification, 0 being extremely negative, 50 being neutral, 100 being extremely positive.\\n\"\n",
    "        \n",
    "        \"Key Strengths Highlighted: Identify the main positive aspects the influencer mentions (e.g., design, performance, technology, comfort, pricing).\\n\"\n",
    "        \n",
    "        \"Key Weaknesses Highlighted: Identify the main negative aspects the influencer mentions (e.g., high price, poor fuel economy, lack of features). \"\n",
    "        \"Do not hesitate to point them out.\\n\"\n",
    "        \n",
    "        \"Comments on Renault Brand: Does the influencer mention Renault as a brand? If so, is the sentiment positive, neutral, or negative?\\n\"\n",
    "        \n",
    "        \"Comparison to Competitors: If the video mentions other car brands/models, list them and summarize the comparisons.\\n\"\n",
    "        \n",
    "        \"Trends & Topics (if available, otherwise return only '/'): Identify any recurring themes (e.g., luxury appeal, fuel efficiency, safety features).\\n\"\n",
    "        \n",
    "        f\"Influencer’s Overall Verdict: Summarize the influencer’s final thoughts on the {model} in one or two sentences, and what is said if compared Renault Korea's Grand Koleos when applicable.\\n\\n\"\n",
    "        \n",
    "        \"Additional Insights to Extract:\\n\"\n",
    "        \n",
    "        \"- Battery Performance: Does the influencer mention issues like fast charging/discharging, battery life, or management system?\\n\"\n",
    "        \"- Noise Levels: Are there comments on the vehicle being too noisy or too quiet at certain speeds?\\n\"\n",
    "        f\"- Competitor Perception: What car is perceived as the {model}'s main competitor?\\n\"\n",
    "        \"- References to Chinese Brands: Does the influencer mention Chinese brands (e.g., BYD, Geely, Haval)? If so, in what context?\\n\"\n",
    "        \n",
    "        \"Here is an example of how the output should be formatted where applicable:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"sentiment_analysis\": {\\n'\n",
    "        '    \"overall_sentiment\": \"Positive\",\\n'\n",
    "        '    \"score\": 85\\n'\n",
    "        '  },\\n'\n",
    "        '  \"key_strengths\": [\"Spacious interior\", \"Fuel efficiency\", \"Modern technology\"],\\n'\n",
    "        '  \"key_weaknesses\": [\"Expensive\", \"Limited color options\"],\\n'\n",
    "        '  \"renault_brand_sentiment\": \"Neutral\",\\n'\n",
    "        '  \"competitor_mentions\": [\\n'\n",
    "        '    {\\n'\n",
    "        '      \"competitor\": \"Toyota RAV4\",\\n'\n",
    "        '      \"comparison_summary\": \"Influencer states that Grand Koleos has a better interior but higher price.\"\\n'\n",
    "        '    }\\n'\n",
    "        '  ],\\n'\n",
    "        '  \"trends\": [\"Luxury appeal\", \"Fuel efficiency\"],\\n'\n",
    "        '  \"battery_performance\": \"Fast discharge rate mentioned as a concern\",\\n'\n",
    "        '  \"noise_levels\": \"Noted as being noisier than competitors at low speeds\",\\n'\n",
    "        '  \"competitor_perception\": \"Compared mainly to Hyundai Tucson\",\\n'\n",
    "        '  \"chinese_brand_mentions\": \"BYD frequently referenced regarding battery technology\",\\n'\n",
    "        '  \"final_verdict\": \"The influencer believes that the Grand Koleos is a premium SUV with great features but might be slightly overpriced.\"\\n'\n",
    "        '}\\n'\n",
    "    )\n",
    "\n",
    "    return instruction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T07:43:22.038616Z",
     "iopub.status.busy": "2025-03-24T07:43:22.038332Z",
     "iopub.status.idle": "2025-03-24T07:44:49.978174Z",
     "shell.execute_reply": "2025-03-24T07:44:49.977386Z",
     "shell.execute_reply.started": "2025-03-24T07:43:22.038592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_analysis(content, company, model):\n",
    "    instruction = instruct(company, model)\n",
    "    full_prompt = f\"{instruction}\\n\\nPrompt: {content}\"\n",
    "    response = gen_model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "# analysis_koleos, analysis_sorento, analysis_santafe = {}, {}, {}\n",
    "\n",
    "# for url, text in tqdm(transcriptions_koleos.items(), desc=\"Analyzing Videos\", unit=\"video\"):\n",
    "#     analysis_koleos[url] = text_analysis(text, car_company_koleos, car_model_koleos)\n",
    "\n",
    "# for url, text in tqdm(transcriptions_sorento.items(), desc=\"Analyzing Videos\", unit=\"video\"):\n",
    "#     analysis_sorento[url] = text_analysis(text, car_company_sorento, car_model_sorento)\n",
    "\n",
    "# for url, text in tqdm(transcriptions_santafe.items(), desc=\"Analyzing Videos\", unit=\"video\"):\n",
    "#     analysis_santafe[url] = text_analysis(text, car_company_santafe, car_model_santafe)\n",
    "\n",
    "analysis_torres = {}\n",
    "\n",
    "for url, text in tqdm(transcriptions_torres.items(), desc=\"Analyzing Videos\", unit=\"video\"):\n",
    "    analysis_torres[url] = text_analysis(text, car_company_torres, car_model_torres)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T07:29:35.708760Z",
     "iopub.status.busy": "2025-03-24T07:29:35.708404Z",
     "iopub.status.idle": "2025-03-24T07:29:35.712456Z",
     "shell.execute_reply": "2025-03-24T07:29:35.711619Z",
     "shell.execute_reply.started": "2025-03-24T07:29:35.708736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for url, result in analysis.items():\n",
    "#     print(url, result[8:-4])\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-24T07:46:33.426870Z",
     "iopub.status.busy": "2025-03-24T07:46:33.426559Z",
     "iopub.status.idle": "2025-03-24T07:46:33.455279Z",
     "shell.execute_reply": "2025-03-24T07:46:33.454402Z",
     "shell.execute_reply.started": "2025-03-24T07:46:33.426846Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Assuming `analysis` is a dictionary containing the URL as the key and JSON results as the value.\n",
    "data_list = []\n",
    "\n",
    "for url, result in analysis_torres.items():\n",
    "    try:\n",
    "        # Convert JSON string to dictionary if needed\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result[8:-4])\n",
    "        \n",
    "        # Flatten nested dictionaries (sentiment analysis, competitor mentions, etc.)\n",
    "        row = {\n",
    "            \"URL\": url,\n",
    "            \"Overall Sentiment\": result.get(\"sentiment_analysis\", {}).get(\"overall_sentiment\", \"N/A\"),\n",
    "            \"Confidence Score\": result.get(\"sentiment_analysis\", {}).get(\"score\", \"N/A\"),\n",
    "            \"Key Strengths\": \", \".join(result.get(\"key_strengths\", [])),\n",
    "            \"Key Weaknesses\": \", \".join(result.get(\"key_weaknesses\", [])),\n",
    "            \"Renault Brand Sentiment\": result.get(\"renault_brand_sentiment\", \"N/A\"),\n",
    "            \"Competitor Mentions\": \", \".join([c.get(\"competitor\", \"N/A\") for c in result.get(\"competitor_mentions\", [])]),\n",
    "            \"Comparison Summary\": \" | \".join([c.get(\"comparison_summary\", \"N/A\") for c in result.get(\"competitor_mentions\", [])]),\n",
    "            \"Trends\": \", \".join(result.get(\"trends\", [])),\n",
    "            \"Battery Performance\": result.get(\"battery_performance\", \"N/A\"),\n",
    "            \"Noise Levels\": result.get(\"noise_levels\", \"N/A\"),\n",
    "            \"Competitor Perception\": result.get(\"competitor_perception\", \"N/A\"),\n",
    "            \"Chinese Brand Mentions\": result.get(\"chinese_brand_mentions\", \"N/A\"),\n",
    "            \"Final Verdict\": result.get(\"final_verdict\", \"N/A\")\n",
    "        }\n",
    "        \n",
    "        data_list.append(row)\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Convert list of dictionaries into DataFrame\n",
    "df2 = pd.DataFrame(data_list)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T07:49:48.270108Z",
     "iopub.status.busy": "2025-03-24T07:49:48.269800Z",
     "iopub.status.idle": "2025-03-24T07:49:48.279658Z",
     "shell.execute_reply": "2025-03-24T07:49:48.278891Z",
     "shell.execute_reply.started": "2025-03-24T07:49:48.270084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torres_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:00:19.001447Z",
     "iopub.status.busy": "2025-03-24T08:00:19.001148Z",
     "iopub.status.idle": "2025-03-24T08:00:19.009385Z",
     "shell.execute_reply": "2025-03-24T08:00:19.008629Z",
     "shell.execute_reply.started": "2025-03-24T08:00:19.001425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "top_videos = df2.merge(torres_df, right_on='Video URL', left_on='URL', how='inner')\n",
    "top_videos[\"Title\"] = top_videos[\"Title\"].str.replace('\"', \"'\", regex=False)\n",
    "# top_videos[\"Title & Link\"] = top_videos.apply(\n",
    "#     lambda x: f'=HYPERLINK(\"{x[\"Video URL\"]}\", \"{x[\"Title\"]}\")',\n",
    "#     axis=1\n",
    "# )\n",
    "top_videos.drop(columns=[\"URL\", \"Title\"], inplace=True)\n",
    "# top_videos.to_excel(\"torres_analysis.xlsx\", index=False)\n",
    "# print(\"Analysis saved to grand_koleos_analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:02:03.687100Z",
     "iopub.status.busy": "2025-03-24T08:02:03.686772Z",
     "iopub.status.idle": "2025-03-24T08:02:03.702599Z",
     "shell.execute_reply": "2025-03-24T08:02:03.701607Z",
     "shell.execute_reply.started": "2025-03-24T08:02:03.687072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sorted_videos = top_videos.sort_values(by=\"Views\", ascending=False)\n",
    "sorted_videos = sorted_videos.drop(columns=[\"Channel ID\"]).reset_index(drop=True)\n",
    "sorted_videos.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona from Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:10:58.156106Z",
     "iopub.status.busy": "2025-03-24T08:10:58.155773Z",
     "iopub.status.idle": "2025-03-24T08:10:58.166385Z",
     "shell.execute_reply": "2025-03-24T08:10:58.165697Z",
     "shell.execute_reply.started": "2025-03-24T08:10:58.156085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:23:54.579798Z",
     "iopub.status.busy": "2025-03-24T08:23:54.579510Z",
     "iopub.status.idle": "2025-03-24T08:23:54.593000Z",
     "shell.execute_reply": "2025-03-24T08:23:54.592182Z",
     "shell.execute_reply.started": "2025-03-24T08:23:54.579775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# comments_df[\"Video URL\"] = \"https://www.youtube.com/watch?v=\" + comments_df[\"Video ID\"]\n",
    "# comments_df = comments_df.drop(columns=[\"Video ID\"])\n",
    "comments_df = comments_df[comments_df['Video URL'].isin(set(sorted_videos['Video URL'].tolist()))]\n",
    "comments_df = comments_df.reset_index(drop=True)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:26:57.453157Z",
     "iopub.status.busy": "2025-03-24T08:26:57.452742Z",
     "iopub.status.idle": "2025-03-24T08:26:57.461761Z",
     "shell.execute_reply": "2025-03-24T08:26:57.461017Z",
     "shell.execute_reply.started": "2025-03-24T08:26:57.453122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comments_dict = comments_df.groupby(\"Video URL\")[\"Comment\"].apply(lambda x: \"\\n\".join(x)).to_dict()\n",
    "len(comments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:46:38.489880Z",
     "iopub.status.busy": "2025-03-24T08:46:38.489580Z",
     "iopub.status.idle": "2025-03-24T08:46:38.494487Z",
     "shell.execute_reply": "2025-03-24T08:46:38.493469Z",
     "shell.execute_reply.started": "2025-03-24T08:46:38.489858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "    You are an AI trained in natural language processing, sentiment analysis, and user profiling. Your task is to analyze the top 100 YouTube comments for each video and extract meaningful insights to build detailed customer/user profiles.\n",
    "    The YouTube video should be about the new KMG Torres Hybrid.\n",
    "    \n",
    "    ### Instructions:\n",
    "    1. **Comment Analysis:**\n",
    "       - Identify common themes, topics, and patterns in the comments.\n",
    "       - Detect recurring words, phrases, or sentiments.\n",
    "       - Categorize comments into positive, negative, and neutral sentiments.\n",
    "    \n",
    "    2. **User Profile Extraction:**\n",
    "       - Determine the key demographic traits (age, gender, location hints, interests) based on language, slang, or references.\n",
    "       - Identify user personas (e.g., casual viewer, enthusiast, expert, new user, loyal fan).\n",
    "       - Detect potential customer needs, pain points, and preferences.\n",
    "    \n",
    "    3. **Engagement Insights:**\n",
    "       - Assess the level of enthusiasm or emotional connection with the content.\n",
    "       - Identify users' expectations, feedback, and suggestions for improvement.\n",
    "       - Highlight any trends in user behavior (e.g., requests for similar content, complaints, praise).\n",
    "    \n",
    "    4. **Summarized User Profiles:**\n",
    "       - Generate 3-5 user personas based on the comment patterns.\n",
    "       - Describe each persona with details such as age group, interests, motivations, and content preferences.\n",
    "       - Provide actionable insights for content creators or businesses to better engage with these user segments.\n",
    "    \n",
    "    ### Output Format:\n",
    "    For each video, provide:\n",
    "    1. **Summary of key themes in the comments**\n",
    "    2. **Breakdown of sentiment analysis (positive/negative/neutral with % if possible)**\n",
    "    3. **List of recurring topics and keywords**\n",
    "    4. **User personas (3-5), including:**\n",
    "       - **Persona Name:** (e.g., \"Tech Enthusiast Tom\")\n",
    "       - **Description:** (Age, interests, typical behavior)\n",
    "       - **Key Motivations:** (Why they engage with this content)\n",
    "       - **Pain Points:** (Challenges or frustrations they mention)\n",
    "       - **Content Preferences:** (What they enjoy or want more of)\n",
    "    \n",
    "    Use concise and insightful language. Ensure the profiles are useful for understanding the audience and improving engagement. Do not make any introduction, comments, or conclusion. Only do the task you're asked.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:46:41.036692Z",
     "iopub.status.busy": "2025-03-24T08:46:41.036407Z",
     "iopub.status.idle": "2025-03-24T08:48:50.859096Z",
     "shell.execute_reply": "2025-03-24T08:48:50.858336Z",
     "shell.execute_reply.started": "2025-03-24T08:46:41.036669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def text_analysis(content, company, model):\n",
    "    full_prompt = f\"{instruction}\\n\\nHere are the comments: {content}\"\n",
    "    response = gen_model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def comment_analysis(comments_dict, company, model):\n",
    "    customer_profiles = {}\n",
    "\n",
    "    for video_id, comments in tqdm(comments_dict.items(), desc=\"Processing Comments\"):\n",
    "        analysis_result = text_analysis(comments, company, model)\n",
    "        customer_profiles[video_id] = analysis_result\n",
    "\n",
    "    return customer_profiles\n",
    "\n",
    "car_company_torres = \"토레스 하이브리드\"\n",
    "car_model_torres = \"KGM\"\n",
    "\n",
    "profiles = comment_analysis(comments_dict, car_company_torres, car_model_torres)\n",
    "#print(profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-24T08:48:53.761733Z",
     "iopub.status.busy": "2025-03-24T08:48:53.761446Z",
     "iopub.status.idle": "2025-03-24T08:48:53.769754Z",
     "shell.execute_reply": "2025-03-24T08:48:53.768867Z",
     "shell.execute_reply.started": "2025-03-24T08:48:53.761710Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:43:16.147759Z",
     "iopub.status.busy": "2025-03-24T08:43:16.147453Z",
     "iopub.status.idle": "2025-03-24T08:43:16.161441Z",
     "shell.execute_reply": "2025-03-24T08:43:16.160697Z",
     "shell.execute_reply.started": "2025-03-24T08:43:16.147736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sorted_videos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:50:59.143559Z",
     "iopub.status.busy": "2025-03-24T08:50:59.143255Z",
     "iopub.status.idle": "2025-03-24T08:50:59.148470Z",
     "shell.execute_reply": "2025-03-24T08:50:59.147676Z",
     "shell.execute_reply.started": "2025-03-24T08:50:59.143536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sorted_videos['Comments Insights'] = sorted_videos[\"Video URL\"].map(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:52:20.614686Z",
     "iopub.status.busy": "2025-03-24T08:52:20.614326Z",
     "iopub.status.idle": "2025-03-24T08:52:20.620843Z",
     "shell.execute_reply": "2025-03-24T08:52:20.619903Z",
     "shell.execute_reply.started": "2025-03-24T08:52:20.614653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sorted_videos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T08:57:27.531437Z",
     "iopub.status.busy": "2025-03-24T08:57:27.531097Z",
     "iopub.status.idle": "2025-03-24T08:57:27.540485Z",
     "shell.execute_reply": "2025-03-24T08:57:27.539660Z",
     "shell.execute_reply.started": "2025-03-24T08:57:27.531406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sorted_videos.to_csv(\"videos.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T09:06:04.563088Z",
     "iopub.status.busy": "2025-03-24T09:06:04.562702Z",
     "iopub.status.idle": "2025-03-24T09:06:13.067458Z",
     "shell.execute_reply": "2025-03-24T09:06:13.066774Z",
     "shell.execute_reply.started": "2025-03-24T09:06:04.563057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_report(sorted_videos):\n",
    "    # Define the columns to analyze\n",
    "    columns = [\n",
    "        'Overall Sentiment', 'Key Strengths', \n",
    "        'Key Weaknesses', 'Competitor Mentions', 'Comparison Summary', \n",
    "        'Trends', 'Battery Performance', 'Noise Levels', 'Competitor Perception', \n",
    "        'Chinese Brand Mentions', 'Final Verdict', 'Comments Insights'\n",
    "    ]\n",
    "    \n",
    "    # Extract data for each column\n",
    "    extracted_data = {col: sorted_videos[col].dropna().tolist() for col in columns if col in sorted_videos}\n",
    "    \n",
    "    # Construct the structured prompt\n",
    "    full_prompt = f\"\"\"\n",
    "    You are an expert analyst. Given the following data extracted from reviews and reports, analyze each category and provide a concise yet insightful summary.\n",
    "    Identify the most common themes, significant insights, and key takeaways for each aspect. For the Comments Insights, I want you to build different customer personas based on the data in Comments.\n",
    "    \n",
    "    Data:\n",
    "    {json.dumps(extracted_data, indent=2)}\n",
    "    \n",
    "    Provide the analysis in the following structured format:\n",
    "    \n",
    "    Overall Sentiment:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Key Strengths:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Key Weaknesses:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Competitor Mentions:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Comparison Summary:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Trends:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Battery Performance:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Noise Levels:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Competitor Perception:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Chinese Brand Mentions:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Final Verdict:\n",
    "    - [Your analysis here]\n",
    "    \n",
    "    Comments Insights:\n",
    "    - [Your analysis here]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = gen_model.generate_content(full_prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "report = generate_report(sorted_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T09:08:58.758679Z",
     "iopub.status.busy": "2025-03-24T09:08:58.758348Z",
     "iopub.status.idle": "2025-03-24T09:08:58.765242Z",
     "shell.execute_reply": "2025-03-24T09:08:58.764420Z",
     "shell.execute_reply.started": "2025-03-24T09:08:58.758654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T09:08:23.878803Z",
     "iopub.status.busy": "2025-03-24T09:08:23.878499Z",
     "iopub.status.idle": "2025-03-24T09:08:27.597280Z",
     "shell.execute_reply": "2025-03-24T09:08:27.596206Z",
     "shell.execute_reply.started": "2025-03-24T09:08:23.878779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T09:12:19.334215Z",
     "iopub.status.busy": "2025-03-24T09:12:19.333856Z",
     "iopub.status.idle": "2025-03-24T09:12:19.338804Z",
     "shell.execute_reply": "2025-03-24T09:12:19.338048Z",
     "shell.execute_reply.started": "2025-03-24T09:12:19.334187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"report.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
    "    txt_file.write(report.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T09:10:52.236716Z",
     "iopub.status.busy": "2025-03-24T09:10:52.236392Z",
     "iopub.status.idle": "2025-03-24T09:10:52.306181Z",
     "shell.execute_reply": "2025-03-24T09:10:52.305334Z",
     "shell.execute_reply.started": "2025-03-24T09:10:52.236691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Extract text content from the response\n",
    "report_text = report.text.strip()\n",
    "\n",
    "# Create a Word document\n",
    "doc = Document()\n",
    "doc.add_heading('Analysis Report', level=1)\n",
    "\n",
    "# Split response into sections\n",
    "for section in report_text.split('\\n\\n'):  # Use extracted text\n",
    "    section = section.strip()\n",
    "    if ':' in section:\n",
    "        title, content = section.split(':', 1)  # Avoid errors with split\n",
    "        doc.add_heading(title.strip(), level=2)\n",
    "        doc.add_paragraph(content.strip())\n",
    "    else:\n",
    "        doc.add_paragraph(section)  # Handle cases where ':' is missing\n",
    "\n",
    "# Save the report\n",
    "doc.save(\"report.docx\")\n",
    "\n",
    "print(\"Report saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T03:12:25.328833Z",
     "iopub.status.busy": "2025-03-20T03:12:25.328365Z",
     "iopub.status.idle": "2025-03-20T03:12:26.089814Z",
     "shell.execute_reply": "2025-03-20T03:12:26.088896Z",
     "shell.execute_reply.started": "2025-03-20T03:12:25.328798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Assuming `analysis_koleos`, `analysis_sorento`, and `analysis_santafe` are dictionaries containing the URL as the key and JSON results as the value.\n",
    "def process_analysis(analysis, model_name):\n",
    "    data_list = []\n",
    "\n",
    "    for url, result in analysis.items():\n",
    "        try:\n",
    "            # Convert JSON string to dictionary if needed\n",
    "            if isinstance(result, str):\n",
    "                result = json.loads(result[8:-4])  # Assuming result is a string, adjusting slicing accordingly\n",
    "            \n",
    "            # Flatten nested dictionaries (sentiment analysis, competitor mentions, etc.)\n",
    "            row = {\n",
    "                \"URL\": url,\n",
    "                \"Overall Sentiment\": result[\"sentiment_analysis\"][\"overall_sentiment\"],\n",
    "                \"Confidence Score\": result[\"sentiment_analysis\"].get(\"confidence_score\", \"N/A\"),\n",
    "                \"Key Strengths\": \", \".join(result.get(\"key_strengths\", [])),\n",
    "                \"Key Weaknesses\": \", \".join(result.get(\"key_weaknesses\", [])),\n",
    "                \"Renault Brand Sentiment\": result.get(\"renault_brand_sentiment\", \"N/A\"),\n",
    "                \"Competitor Mentions\": \", \".join([c[\"competitor\"] for c in result.get(\"competitor_mentions\", [])]),\n",
    "                \"Trends\": \", \".join(result.get(\"trends\", [])),\n",
    "                \"Final Verdict\": result.get(\"final_verdict\", \"\")\n",
    "            }\n",
    "            \n",
    "            data_list.append(row)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "    \n",
    "    # Convert list of dictionaries into DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process the analysis data for each car model\n",
    "df_koleos = process_analysis(analysis_koleos, \"Koleos\")\n",
    "df_sorento = process_analysis(analysis_sorento, \"Sorento\")\n",
    "df_santafe = process_analysis(analysis_santafe, \"Santa Fe\")\n",
    "\n",
    "# Create an Excel file with three sheets\n",
    "with pd.ExcelWriter(\"grand_koleos_analysis.xlsx\") as writer:\n",
    "    df_koleos.to_excel(writer, sheet_name=\"Koleos\", index=False)\n",
    "    df_sorento.to_excel(writer, sheet_name=\"Sorento\", index=False)\n",
    "    df_santafe.to_excel(writer, sheet_name=\"Santa Fe\", index=False)\n",
    "\n",
    "print(\"Analysis saved to grand_koleos_analysis.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"torres_analysis.xlsx\") as writer:\n",
    "    df_koleos.to_excel(writer, sheet_name=\"Torres\", index=False)\n",
    "\n",
    "print(\"Analysis saved to grand_koleos_analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:11:16.676181Z",
     "iopub.status.busy": "2025-03-20T06:11:16.675887Z",
     "iopub.status.idle": "2025-03-20T06:11:16.749809Z",
     "shell.execute_reply": "2025-03-20T06:11:16.748890Z",
     "shell.execute_reply.started": "2025-03-20T06:11:16.676159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/kaggle/input/competitors/koleos-competitors.xlsx'\n",
    "\n",
    "koleos = pd.read_excel(file_path, sheet_name=0, header=1, usecols=lambda x: x != 0).iloc[:, 1:]\n",
    "sorento = pd.read_excel(file_path, sheet_name=1, header=1, usecols=lambda x: x != 0).iloc[:, 1:]\n",
    "santafe = pd.read_excel(file_path, sheet_name=2, header=1, usecols=lambda x: x != 0).iloc[:, 1:]\n",
    "\n",
    "koleos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:11:18.462854Z",
     "iopub.status.busy": "2025-03-20T06:11:18.462505Z",
     "iopub.status.idle": "2025-03-20T06:11:20.096327Z",
     "shell.execute_reply": "2025-03-20T06:11:20.095455Z",
     "shell.execute_reply.started": "2025-03-20T06:11:18.462828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Set up YouTube Data API client\n",
    "user_secrets = UserSecretsClient()\n",
    "API_KEY = user_secrets.get_secret(\"API_KEY3\")\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r'(https?://www\\.youtube\\.com/watch\\?v=)([a-zA-Z0-9_-]+)', url)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    return None\n",
    "\n",
    "# Function to fetch video details from YouTube API\n",
    "def get_video_details(video_id):\n",
    "    try:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            id=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        video_info = response['items'][0]\n",
    "        \n",
    "        # Extract details from video_info\n",
    "        title = video_info['snippet']['title']\n",
    "        release_date = video_info['snippet']['publishedAt']\n",
    "        channel_id = video_info['snippet'].get('channelId', 'N/A')\n",
    "        channel_title = video_info['snippet'].get('channelTitle', 'N/A')\n",
    "        views = int(video_info['statistics'].get('viewCount', 0))\n",
    "        likes = int(video_info['statistics'].get('likeCount', 0))\n",
    "        comments = int(video_info['statistics'].get('commentCount', 0))\n",
    "        duration = video_info['contentDetails'].get('duration', 'N/A')\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'release_date': release_date,\n",
    "            'channel_id': channel_id,\n",
    "            'channel_title': channel_title,\n",
    "            'views': views,\n",
    "            'likes': likes,\n",
    "            'comments': comments,\n",
    "            'duration': duration\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for video ID {video_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add video details to the DataFrame\n",
    "def add_video_details_to_df(df):\n",
    "    video_details = []\n",
    "    for index, row in df.iterrows():\n",
    "        video_id = extract_video_id(row['URL'])\n",
    "        if video_id:\n",
    "            details = get_video_details(video_id)\n",
    "            if details:\n",
    "                video_details.append(details)\n",
    "            else:\n",
    "                video_details.append({col: 'N/A' for col in ['title', 'release_date', 'channel_id', 'channel_title', 'views', 'likes', 'comments', 'duration']})\n",
    "        else:\n",
    "            video_details.append({col: 'N/A' for col in ['title', 'release_date', 'channel_id', 'channel_title', 'views', 'likes', 'comments', 'duration']})\n",
    "\n",
    "    # Convert the video details to a DataFrame and merge it with the original DataFrame\n",
    "    video_details_df = pd.DataFrame(video_details)\n",
    "    df = pd.concat([df, video_details_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# Applying the function to each DataFrame\n",
    "koleos = add_video_details_to_df(koleos)\n",
    "sorento = add_video_details_to_df(sorento)\n",
    "santafe = add_video_details_to_df(santafe)\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(\"Koleos DataFrame:\")\n",
    "print(koleos.head())\n",
    "\n",
    "print(\"Sorento DataFrame:\")\n",
    "print(sorento.head())\n",
    "\n",
    "print(\"Santafe DataFrame:\")\n",
    "print(santafe.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:11:20.889027Z",
     "iopub.status.busy": "2025-03-20T06:11:20.888734Z",
     "iopub.status.idle": "2025-03-20T06:11:20.907554Z",
     "shell.execute_reply": "2025-03-20T06:11:20.906710Z",
     "shell.execute_reply.started": "2025-03-20T06:11:20.889006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to format release_date, duration, likes, comments\n",
    "def format_video_details(df):\n",
    "    # 1. Change the release_date to 'YYYY-MM-DD HH:MM' format\n",
    "    df['release_date'] = pd.to_datetime(df['release_date']).dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    # 2. Change duration to hour, minute, second format (assuming it’s in ISO 8601 format, e.g., PT32S or PT1H5M)\n",
    "    def format_duration(duration):\n",
    "        # Check for duration pattern and convert it to hours, minutes, and seconds\n",
    "        if duration.startswith('PT'):\n",
    "            duration = duration[2:]  # Remove the 'PT' part\n",
    "            \n",
    "            hours, minutes, seconds = 0, 0, 0\n",
    "            \n",
    "            # Extract hours, minutes, seconds\n",
    "            if 'H' in duration:\n",
    "                hours = int(duration.split('H')[0])\n",
    "                duration = duration.split('H')[1]\n",
    "            if 'M' in duration:\n",
    "                minutes = int(duration.split('M')[0].split('H')[-1])\n",
    "                duration = duration.split('M')[1] if 'S' in duration else ''\n",
    "            if 'S' in duration:\n",
    "                seconds = int(duration.split('S')[0].split('M')[-1])\n",
    "            \n",
    "            # Format as \"HH:MM:SS\"\n",
    "            return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "        \n",
    "        return '00:00:00'  # If duration is missing or invalid\n",
    "\n",
    "    df['duration'] = df['duration'].apply(format_duration)\n",
    "\n",
    "    # 3. Remove 'channel_id' column\n",
    "    df = df.drop(columns=['channel_id'])\n",
    "\n",
    "    # 4. Format likes and comments to have commas every thousand\n",
    "    df['likes'] = df['likes'].apply(lambda x: f\"{x:,}\" if pd.notnull(x) else 'N/A')\n",
    "    df['comments'] = df['comments'].apply(lambda x: f\"{x:,}\" if pd.notnull(x) else 'N/A')\n",
    "    df['views'] = df['views'].apply(lambda x: f\"{x:,}\" if pd.notnull(x) else 'N/A')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply transformations to each DataFrame\n",
    "koleos = format_video_details(koleos)\n",
    "sorento = format_video_details(sorento)\n",
    "santafe = format_video_details(santafe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:13:21.048322Z",
     "iopub.status.busy": "2025-03-20T06:13:21.048037Z",
     "iopub.status.idle": "2025-03-20T06:13:21.063342Z",
     "shell.execute_reply": "2025-03-20T06:13:21.062325Z",
     "shell.execute_reply.started": "2025-03-20T06:13:21.048301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "koleos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:24:51.705333Z",
     "iopub.status.busy": "2025-03-20T06:24:51.704994Z",
     "iopub.status.idle": "2025-03-20T06:24:51.755107Z",
     "shell.execute_reply": "2025-03-20T06:24:51.754423Z",
     "shell.execute_reply.started": "2025-03-20T06:24:51.705307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to modify the DataFrame\n",
    "def modify_dataframe(df):\n",
    "    # 1. Remove 'title' and 'URL' columns, create a new 'Title & Link' column\n",
    "    df[\"Title & Link\"] = df.apply(\n",
    "        lambda x: '=HYPERLINK(\"{0}\", \"{1}\")'.format(x[\"URL\"], str(x[\"title\"]).replace(\"\\\"\", \"\\\"\\\"\")) \n",
    "        if pd.notnull(x[\"URL\"]) and pd.notnull(x[\"title\"]) else 'N/A',  # Ensure both URL and title are present\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop the original 'title' and 'URL' columns\n",
    "    df = df.drop(columns=['title', 'URL'], errors='ignore')\n",
    "\n",
    "    # 2. Reorder the columns\n",
    "    ordered_columns = [\n",
    "        \"Title & Link\", \"release_date\", \"channel_title\", \"Overall Sentiment\", \n",
    "        \"Key Strengths\", \"Key Weaknesses\", \"Renault Brand Sentiment\", \n",
    "        \"Competitor Mentions\", \"Trends\", \"Final Verdict\", \"views\", \"likes\", \n",
    "        \"comments\", \"duration\"\n",
    "    ]\n",
    "    df = df[ordered_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Modify the dataframes\n",
    "koleos_modified = modify_dataframe(koleos)\n",
    "sorento_modified = modify_dataframe(sorento)\n",
    "santafe_modified = modify_dataframe(santafe)\n",
    "\n",
    "# 3. Save each dataframe to a different sheet in an Excel file\n",
    "with pd.ExcelWriter('competitors.xlsx') as writer:\n",
    "    koleos_modified.to_excel(writer, sheet_name='Koleos', index=False)\n",
    "    sorento_modified.to_excel(writer, sheet_name='Sorento', index=False)\n",
    "    santafe_modified.to_excel(writer, sheet_name='Santafe', index=False)\n",
    "\n",
    "print(\"DataFrames saved successfully to 'competitors.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:39:12.711379Z",
     "iopub.status.busy": "2025-03-20T06:39:12.711044Z",
     "iopub.status.idle": "2025-03-20T06:39:13.339876Z",
     "shell.execute_reply": "2025-03-20T06:39:13.339039Z",
     "shell.execute_reply.started": "2025-03-20T06:39:12.711355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to get sentiment counts for a dataframe\n",
    "def get_sentiment_counts(df, df_name):\n",
    "    # Count the frequencies of each sentiment in the 'Overall Sentiment' column\n",
    "    sentiment_counts = df['Overall Sentiment'].value_counts()\n",
    "    # Create a DataFrame with the sentiment counts and add the DataFrame name as a column\n",
    "    sentiment_df = sentiment_counts.reset_index()\n",
    "    sentiment_df.columns = ['Sentiment', df_name]  # Rename columns\n",
    "    return sentiment_df\n",
    "\n",
    "# Get sentiment counts for each dataframe\n",
    "koleos_sentiment = get_sentiment_counts(koleos, 'Koleos')\n",
    "sorento_sentiment = get_sentiment_counts(sorento, 'Sorento')\n",
    "santafe_sentiment = get_sentiment_counts(santafe, 'Santafe')\n",
    "\n",
    "# Merge the dataframes on the 'Sentiment' column\n",
    "combined_sentiment = pd.merge(koleos_sentiment, sorento_sentiment, on='Sentiment', how='outer')\n",
    "combined_sentiment = pd.merge(combined_sentiment, santafe_sentiment, on='Sentiment', how='outer')\n",
    "\n",
    "# Plot the combined sentiment distribution as a bar chart\n",
    "combined_sentiment.set_index('Sentiment', inplace=True)\n",
    "\n",
    "# Plot\n",
    "combined_sentiment.plot(kind='bar', figsize=(10, 6), color=['skyblue', 'lightgreen', 'salmon'])\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Overall Sentiment Distribution Comparison', fontsize=14)\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=0)  # Keeps the sentiment labels horizontal\n",
    "plt.tight_layout()\n",
    "plt.legend(title='DataFrames', fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T06:42:29.258806Z",
     "iopub.status.busy": "2025-03-20T06:42:29.258452Z",
     "iopub.status.idle": "2025-03-20T06:42:30.121883Z",
     "shell.execute_reply": "2025-03-20T06:42:30.121079Z",
     "shell.execute_reply.started": "2025-03-20T06:42:29.258779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to get sentiment counts for a dataframe\n",
    "def get_sentiment_counts(df, df_name):\n",
    "    sentiment_counts = df['Overall Sentiment'].value_counts()\n",
    "    sentiment_df = sentiment_counts.reset_index()\n",
    "    sentiment_df.columns = ['Sentiment', df_name]\n",
    "    return sentiment_df\n",
    "\n",
    "# Replace \"Neutral to Slightly Negative\" with \"Negative\" in the Overall Sentiment column\n",
    "def replace_sentiment(df):\n",
    "    df['Overall Sentiment'] = df['Overall Sentiment'].replace('Neutral to Slightly Negative', 'Negative')\n",
    "    return df\n",
    "\n",
    "# Replace sentiment in each dataframe\n",
    "koleos = replace_sentiment(koleos)\n",
    "sorento = replace_sentiment(sorento)\n",
    "santafe = replace_sentiment(santafe)\n",
    "\n",
    "# Get sentiment counts for each dataframe\n",
    "koleos_sentiment = get_sentiment_counts(koleos, 'Koleos')\n",
    "sorento_sentiment = get_sentiment_counts(sorento, 'Sorento')\n",
    "santafe_sentiment = get_sentiment_counts(santafe, 'Santafe')\n",
    "\n",
    "# Merge the dataframes on the 'Sentiment' column\n",
    "combined_sentiment = pd.merge(koleos_sentiment, sorento_sentiment, on='Sentiment', how='outer')\n",
    "combined_sentiment = pd.merge(combined_sentiment, santafe_sentiment, on='Sentiment', how='outer')\n",
    "\n",
    "# Set the sentiment as the index for easy plotting\n",
    "combined_sentiment.set_index('Sentiment', inplace=True)\n",
    "\n",
    "# Set up the color palette using Seaborn for a more pleasant appearance\n",
    "colors = sns.color_palette(\"coolwarm\", len(combined_sentiment.columns))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a bar plot with improved visual elements\n",
    "combined_sentiment.plot(kind='bar', width=0.8, color=colors, edgecolor='black', figsize=(12, 8))\n",
    "\n",
    "# Customize the plot with a more beautiful design\n",
    "plt.title('Overall Sentiment Distribution Comparison', fontsize=16, fontweight='bold', color='darkblue')\n",
    "plt.xlabel('Sentiment', fontsize=14, fontweight='bold', color='darkblue')\n",
    "plt.ylabel('Frequency', fontsize=14, fontweight='bold', color='darkblue')\n",
    "\n",
    "# Rotate x-ticks for better readability\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Customize the legend and position it outside the plot\n",
    "plt.legend(title='DataFrames', fontsize=12, title_fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Ensure the layout fits well\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_distribution_comparison.png', dpi=300)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6918305,
     "sourceId": 11098270,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
